{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Make RGB NIRCam stamps showing slit footprint\n",
    "We will be using the `trilogy` package to make RGB stamps and the shutter footprint regions.\n",
    "The information we need to show the footprint and the source (optional) is all in the \"shutters\" files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trilogy import trilogy\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import astropy.units as u\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL  # Python Image Library\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "import contextlib\n",
    "\n",
    "from astropy.nddata import Cutout2D\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Read in all the data and specify the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- SETUP ----------\n",
    "input_dir = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/\"\n",
    "output_dir = \"/home/bpc/University/master/Red_Cardinal/stamps/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ---------- STEP 1: Get IDs from NIRCam, PRIMER and COSMOS-Web ----------\n",
    "f444w_files = glob.glob(os.path.join(input_dir, \"*F444W*.fits\"))\n",
    "f444w_ids = [os.path.basename(f).split(\"_F444W\")[0] for f in f444w_files]\n",
    "\n",
    "f770w_primer_files = glob.glob(os.path.join(input_dir, \"*F770W_cutout_primer.fits\"))\n",
    "f770w_primer_ids = [os.path.basename(f).split(\"_F770W\")[0] for f in f770w_primer_files]\n",
    "\n",
    "f770w_cweb_files = glob.glob(os.path.join(input_dir, \"*F770W_cutout_cweb.fits\"))\n",
    "f770w_cweb_ids = [os.path.basename(f).split(\"_F770W\")[0] for f in f770w_cweb_files]\n",
    "\n",
    "f1800w_files = glob.glob(os.path.join(input_dir, \"*F1800W*.fits\"))\n",
    "f1800w_ids = [os.path.basename(f).split(\"_F1800W\")[0] for f in f1800w_files]\n",
    "\n",
    "# Remove all galaxies in f770w_primer_ids from f770w_cweb_ids\n",
    "f770w_cweb_ids = np.setdiff1d(f770w_cweb_ids, f770w_primer_ids)\n",
    "\n",
    "\n",
    "# ---------- STEP 2: Collect MIRI and NIRCam Files ----------\n",
    "fits_files = glob.glob(os.path.join(input_dir, \"*.fits\"))\n",
    "filters = [\"F444W\", \"F770W\", \"F1800W\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Function to produce the RGB-stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_image(img, stretch='asinh', Q=10, alpha=1, weight=1.0):\n",
    "    \"\"\"\n",
    "    Normalises the input image with optional stretching and channel weighting.\n",
    "\n",
    "    Parameters:\n",
    "    - img: 2D numpy array\n",
    "    - stretch: Type of stretch ('asinh' or 'linear')\n",
    "    - Q: Controls asinh stretch strength\n",
    "    - alpha: Controls non-linearity for asinh\n",
    "    - weight: Multiplier to boost/dampen this channelâ€™s contribution\n",
    "\n",
    "    Returns:\n",
    "    - Normalised image scaled between 0 and 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace nans, positive and negative infinities with 0.0\n",
    "    img = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Clip possible negative fluxes\n",
    "    img = np.clip(img, 0, None)\n",
    "    \n",
    "    #print(f\"Before any scaling: min={np.min(img)}, max={np.max(img)}\")\n",
    "    \n",
    "    # Subtract the minimum value to shift the range to start from 0\n",
    "    img_min = np.min(img)\n",
    "    img -= img_min\n",
    "    #print(f\"After minimum subtraction: min={np.min(img)}, max={np.max(img)}\")\n",
    "\n",
    "    \n",
    "    #print(f\"After weight scaling: min={np.min(img)}, max={np.max(img)}\")\n",
    "    \n",
    "    # Determine which scaling to us\n",
    "    if stretch == 'asinh':  # Lupton scaling\n",
    "        img_scaled = np.arcsinh(alpha * Q * img) / Q\n",
    "    elif stretch == 'log':\n",
    "        img_scaled = np.log10(1 + alpha * img)\n",
    "    elif stretch == 'linear':\n",
    "        img_scaled = img\n",
    "    else:\n",
    "        raise ValueError(\"Unknown stretch\")\n",
    "        \n",
    "    #print(f\"After stretch: min={np.min(img_scaled)}, max={np.max(img_scaled)}\")\n",
    "    \n",
    "    # After stretching the image is normalised to 1\n",
    "    img_scaled = img_scaled / np.nanmax(img_scaled) if np.nanmax(img_scaled) != 0 else img_scaled\n",
    "    \n",
    "    #print(f\"After normalisation: min={np.min(img_scaled)}, max={np.max(img_scaled)}\")\n",
    "    \n",
    "    return img_scaled\n",
    "\n",
    "\n",
    "def preprocess_fits_image(filename, ext=0, stretch='asinh', Q=10, alpha=1, weight=1, normalise=False):\n",
    "    \"\"\"\n",
    "    Load, optionally resample, and normalise a FITS image.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: FITS filename with optional extension (e.g., 'file.fits[1]')\n",
    "    - stretch, Q, alpha, weight: Passed to normalize_image\n",
    "    - upscale_size: Tuple of (new_y, new_x) size to resize image to\n",
    "\n",
    "    Returns:\n",
    "    - Processed 2D numpy image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fits.open(filename) as hdul:\n",
    "            img = hdul[ext].data.astype(float)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Could not open {filename}[{ext}]: {e}\")\n",
    "\n",
    "    # Remove NaNs and negative values for display purposes\n",
    "    img_min = np.min(img)\n",
    "    img -= img_min\n",
    "    img = np.nan_to_num(img, nan=0.0)\n",
    "    img[img < 0] = 0.0\n",
    "\n",
    "    # Apply stretch\n",
    "    if stretch == 'asinh':\n",
    "        img = np.arcsinh(Q * alpha * img) / Q\n",
    "    elif stretch == 'linear':\n",
    "        pass  # No stretch applied\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported stretch: {stretch}\")\n",
    "\n",
    "    # Normalise if requested\n",
    "    if normalise:\n",
    "        max_val = np.nanmax(img)\n",
    "        if max_val > 0:\n",
    "            img /= max_val\n",
    "\n",
    "    # Clip just in case\n",
    "    #img = np.clip(img, 0, 1)\n",
    "\n",
    "    return img\n",
    "    \n",
    "\n",
    "def make_stamp(imagesRGB, Q_r, alpha_r, weight_r, Q_g, alpha_g, weight_g, Q_b, alpha_b, weight_b=1.0, stretch='asinh', outfile='stamp.pdf'):\n",
    "    \"\"\"\n",
    "    Make RGB stamp using trilogy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameter dictionary to handle values per channel\n",
    "    params = {\n",
    "        'R': {'Q': Q_r, 'alpha': alpha_r, 'weight': weight_r},\n",
    "        'G': {'Q': Q_g, 'alpha': alpha_g, 'weight': weight_g},\n",
    "        'B': {'Q': Q_b, 'alpha': alpha_b, 'weight': weight_b}\n",
    "    }\n",
    "    \n",
    "    stretched_images = {}\n",
    "    global_max = 0.0\n",
    "    \n",
    "    # Stretch each image and find the global max\n",
    "    for colour in ['R', 'G', 'B']:\n",
    "        image_str = imagesRGB[colour][0]\n",
    "        fname, ext = image_str.split('[')\n",
    "        ext = int(ext.replace(']', ''))\n",
    "        colour_params = params[colour]\n",
    "\n",
    "        # Stretch but don't normalise yet\n",
    "        stretched = preprocess_fits_image(\n",
    "            fname,\n",
    "            ext,\n",
    "            stretch=stretch,\n",
    "            Q=colour_params['Q'],\n",
    "            alpha=colour_params['alpha'],\n",
    "            normalise=False  # You'll need to add this option in your function\n",
    "        )\n",
    "\n",
    "        stretched_images[colour] = stretched\n",
    "        max_val = np.nanmax(stretched)\n",
    "        if max_val > global_max:\n",
    "            global_max = max_val\n",
    "\n",
    "    print(f\"[INFO] Global max across channels: {global_max}\")\n",
    "    \n",
    "    # Now normalise to global max\n",
    "    norm_images = {}\n",
    "    for colour in ['R', 'G', 'B']:\n",
    "        norm_images[colour] = stretched_images[colour] / global_max\n",
    "        norm_images[colour] = np.clip(norm_images[colour], 0, 1)\n",
    "\n",
    "    # Add scaled images to the dictionary\n",
    "    temp_files = {}\n",
    "    for colour in norm_images:\n",
    "        fname = f'temp_{colour}.fits'\n",
    "        fits.writeto(fname, norm_images[colour], overwrite=True)\n",
    "        temp_files[colour] = fname\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(norm_images['R'], cmap='Reds', origin='lower')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(norm_images['G'], cmap='Greens', origin='lower')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(norm_images['B'], cmap='Blues', origin='lower')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # set luminosity of the noise in each channel\n",
    "    noiselums = {'R': 0.1, 'G': 0.1, 'B': 0.2}\n",
    "    \"\"\"\n",
    "    with open(os.devnull, \"w\") as fnull, contextlib.redirect_stdout(fnull): # avoid printing out stuff\n",
    "        trilogy.Trilogy(\n",
    "            infile=None, samplesize=20000, stampsize=20000, maxstampsize=20000,\n",
    "            deletetests=1, deletefilters=1, testfirst=0, showwith=\"PIL\",\n",
    "            mode='RGB', imagesorder='RGB',\n",
    "            imagesRGB={'R': [temp_files['R']], 'G': [temp_files['G']], 'B': [temp_files['B']]},\n",
    "            noiselums=noiselums, images=None, outname='color_image_temp', satpercent=0.000, \n",
    "            noiselum=0.5, noisesig0=10, correctbias=0, colorsatfac=1, combine='sum', show=False\n",
    "            ).run()\n",
    "    \n",
    "    # read in RGB image and delete png file\n",
    "    im = Image.open('color_image_temp.png')\n",
    "    im = im.transpose(method=Image.FLIP_TOP_BOTTOM)\n",
    "    NIRCam_image = np.asarray(im)\n",
    "    os.remove('color_image_temp.png')\n",
    "    \n",
    "    # read WCS from a single FITS file\n",
    "    single_filename = imagesRGB['R'][0].split('[')[0] # remove '[1]' if present\n",
    "    image_hdulist = fits.open(single_filename)\n",
    "    image_wcs = WCS(image_hdulist[1].header, image_hdulist)\n",
    "    \"\"\"\n",
    "\n",
    "    # Legend info\n",
    "    legend_text = f\"R: {os.path.basename(imagesRGB['R'][0]).split('_')[1]}\\n\" \\\n",
    "                  f\"G: {os.path.basename(imagesRGB['G'][0]).split('_')[1]}\\n\" \\\n",
    "                  f\"B: {os.path.basename(imagesRGB['B'][0]).split('_')[1]}\"\n",
    "    \n",
    "    # --- Plot using matplotlib and save ---\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    rgb_image = np.stack([norm_images['R'], norm_images['G'], norm_images['B']], axis=-1)\n",
    "    ax.imshow(rgb_image, origin='lower')\n",
    "    #ax.imshow(NIRCam_image, origin='lower')\n",
    "    ax.text(0.02, 0.98, legend_text, transform=ax.transAxes,\n",
    "            fontsize=10, va='top', ha='left',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "    ax.axis('off')  # Hide axes\n",
    "    \n",
    "    # Save the RGB image as PDF\n",
    "    fig.savefig(outfile, bbox_inches='tight', pad_inches=0.0)\n",
    "    plt.close(fig)  # Clean up the figure\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Start making the stamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Function which calls the make_stamps function iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for gal_id in f1800w_ids:\n",
    "    try:\n",
    "        print(f\"Creating image for {gal_id}...\")\n",
    "        \n",
    "        # Construct base filenames (without [0])\n",
    "        r_file_base = os.path.join(input_dir, f\"{gal_id}_F1800W_cutout_primer.fits\")\n",
    "        g_file_primer_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_primer.fits\")\n",
    "        g_file_cweb_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_cweb.fits\")\n",
    "        b_file_base = os.path.join(input_dir, f\"{gal_id}_F444W_cutout_nircam.fits\")\n",
    "\n",
    "        # Decide which F770W file to use\n",
    "        if gal_id in f770w_primer_ids and os.path.exists(g_file_primer_base):\n",
    "            g_file = g_file_primer_base + \"[1]\"\n",
    "        elif gal_id in f770w_cweb_ids and os.path.exists(g_file_cweb_base):\n",
    "            g_file = g_file_cweb_base + \"[1]\"\n",
    "        else:\n",
    "            print(f\"[Skipping {gal_id}] No valid F770W file found.\")\n",
    "            #continue\n",
    "\n",
    "        # Now safely add [0] to files\n",
    "        r_file = r_file_base + \"[1]\"\n",
    "        b_file = b_file_base + \"[0]\"\n",
    "        \n",
    "        # Stack into imagesRGB dictionary\n",
    "        imagesRGB = {'R': [r_file], \n",
    "                    'G': [g_file], \n",
    "                    'B': [b_file]}\n",
    "        \n",
    "        #print(imagesRGB)\n",
    "        \n",
    "        # Call your image function (without shutters)\n",
    "        # Q determines the strength of the non-linearity for the stretch.\n",
    "        # alpha adjusts the sensitivity of the stretch for the given filter.\n",
    "\n",
    "        outfile = os.path.join(output_dir, f\"{gal_id}_stamp.pdf\")\n",
    "        make_stamp(imagesRGB, outfile=outfile, \n",
    "                Q_r=10, alpha_r=1.0, weight_r=1,\n",
    "                Q_g=10, alpha_g=1.0, weight_g=1,\n",
    "                Q_b=10, alpha_b=1.0, weight_b=1,\n",
    "                stretch='asinh')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gal_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nircam_fits = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/8338_F444W_cutout_3x3.fits\"\n",
    "miri_770_fits = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/8338_F770W_cutout_primer004.fits\"\n",
    "miri_1800_fits = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/8338_F1800W_cutout_primer004.fits\"\n",
    "\n",
    "with fits.open(nircam_fits) as nircam_hdul:\n",
    "    nircam_data = nircam_hdul[0].data\n",
    "    print(nircam_data)\n",
    "\n",
    "with fits.open(miri_770_fits) as miri_770_hdul:\n",
    "    miri_770_data = miri_770_hdul[1].data\n",
    "    print(miri_770_data)\n",
    "\n",
    "with fits.open(miri_1800_fits) as miri_1800_hdul:\n",
    "    miri_1800_data = miri_1800_hdul[1].data\n",
    "    print(miri_1800_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Check extensions on cutout FITS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_file = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/18769_F1800W_cutout_primer.fits\"\n",
    "with fits.open(r_file) as hdul:\n",
    "    for i, hdu in enumerate(hdul):\n",
    "        print(f\"HDU {i}: {hdu.name}, shape = {getattr(hdu.data, 'shape', None)}\")\n",
    "print(\"\\n\")\n",
    "g_file = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/18769_F770W_cutout_primer.fits\"\n",
    "with fits.open(g_file) as hdul:\n",
    "    for i, hdu in enumerate(hdul):\n",
    "        print(f\"HDU {i}: {hdu.name}, shape = {getattr(hdu.data, 'shape', None)}\")\n",
    "print(\"\\n\")        \n",
    "b_file = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/18769_F444W_cutout_nircam.fits\"\n",
    "with fits.open(b_file) as hdul:\n",
    "    for i, hdu in enumerate(hdul):\n",
    "        print(f\"HDU {i}: {hdu.name}, shape = {getattr(hdu.data, 'shape', None)}\")\n",
    "print(\"\\n\")\n",
    "shifted_file = \"/home/bpc/University/master/Red_Cardinal/cutouts_shifted/7102_F770W_cutout_cweb1_shifted.fits\"\n",
    "with fits.open(shifted_file) as hdul:\n",
    "    for i, hdu in enumerate(hdul):\n",
    "        print(f\"HDU {i}: {hdu.name}, shape = {getattr(hdu.data, 'shape', None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Alternative function that produces RGB images from the 5x5\" images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SETUP ----------\n",
    "input_dir = \"/home/bpc/University/master/Red_Cardinal/cutouts_shifted/\"\n",
    "output_dir = \"/home/bpc/University/master/Red_Cardinal/stamps_v3/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ---------- STEP 1: Get IDs from NIRCam, PRIMER and COSMOS-Web ----------\n",
    "f770w_primer003_files = glob.glob(os.path.join(input_dir, \"*F770W_cutout_primer003_shifted.fits\"))\n",
    "f770w_primer003_ids = [os.path.basename(f).split(\"_F770W\")[0] for f in f770w_primer003_files]\n",
    "\n",
    "f770w_primer004_files = glob.glob(os.path.join(input_dir, \"*F770W_cutout_primer004_shifted.fits\"))\n",
    "f770w_primer004_ids = [os.path.basename(f).split(\"_F770W\")[0] for f in f770w_primer004_files]\n",
    "\n",
    "f770w_cweb1_files = glob.glob(os.path.join(input_dir, \"*F770W_cutout_cweb1_shifted.fits\"))\n",
    "f770w_cweb1_ids = [os.path.basename(f).split(\"_F770W\")[0] for f in f770w_cweb1_files]\n",
    "\n",
    "f770w_cweb2_files = glob.glob(os.path.join(input_dir, \"*F770W_cutout_cweb2_shifted.fits\"))\n",
    "f770w_cweb2_ids = [os.path.basename(f).split(\"_F770W\")[0] for f in f770w_cweb2_files]\n",
    "\n",
    "f1800w_primer003_files = glob.glob(os.path.join(input_dir, \"*F1800W_cutout_primer003_shifted.fits\"))\n",
    "f1800w_primer003_ids = [os.path.basename(f).split(\"_F1800W\")[0] for f in f1800w_primer003_files]\n",
    "\n",
    "f1800w_primer004_files = glob.glob(os.path.join(input_dir, \"*F1800W_cutout_primer004_shifted.fits\"))\n",
    "f1800w_primer004_ids = [os.path.basename(f).split(\"_F1800W\")[0] for f in f1800w_primer004_files]\n",
    "\n",
    "# Remove all galaxies from COSMOS-Web ID list that are covered by PRIMER in the F770W band\n",
    "f770w_cweb1_ids = np.setdiff1d(f770w_cweb1_ids, f770w_primer003_ids)\n",
    "f770w_cweb1_ids = np.setdiff1d(f770w_cweb1_ids, f770w_primer004_ids)\n",
    "\n",
    "f770w_cweb2_ids = np.setdiff1d(f770w_cweb2_ids, f770w_primer003_ids)\n",
    "f770w_cweb2_ids = np.setdiff1d(f770w_cweb2_ids, f770w_primer004_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Define the new make_stamp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save_temp_fits(imagesRGB):\n",
    "    temp_imagesRGB = {'R': [], 'G': [], 'B': []}\n",
    "    for colour in ['R', 'G', 'B']:\n",
    "        for entry in imagesRGB[colour]:\n",
    "            filename, ext = entry.strip(']').split('[')\n",
    "            ext = int(ext)\n",
    "            with fits.open(filename) as hdul:\n",
    "                data = hdul[ext].data\n",
    "                data = np.nan_to_num(data)\n",
    "                norm_data = (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-8)\n",
    "                # Create temp FITS file\n",
    "                hdu = fits.PrimaryHDU(norm_data, header=hdul[ext].header)\n",
    "                temp_file = tempfile.NamedTemporaryFile(suffix='.fits', delete=False)\n",
    "                hdu.writeto(temp_file.name, overwrite=True)\n",
    "                temp_imagesRGB[colour].append(f\"{temp_file.name}[0]\")\n",
    "    return temp_imagesRGB\n",
    "\n",
    "def make_stamp(imagesRGB, outfile='stamp.pdf', crop_arcsec=3.0, show_channels=False):\n",
    "    \"\"\"\n",
    "    Make RGB stamp using trilogy and overplot the footprint of the shutters\n",
    "    \"\"\"\n",
    "        \n",
    "    # set luminosity of the noise in each channel\n",
    "    noiselums = {'R': 0.2, 'G': 0.2, 'B': 0.2}\n",
    "    \n",
    "    imagesRGB = clean_and_save_temp_fits(imagesRGB)\n",
    "\n",
    "    \n",
    "    with open(os.devnull, \"w\") as fnull, contextlib.redirect_stdout(fnull): # avoid printing out stuff\n",
    "        trilogy.Trilogy(\n",
    "            infile=None, samplesize=20000, stampsize=20000, maxstampsize=20000,\n",
    "            deletetests=1, deletefilters=1, testfirst=0, showwith=\"PIL\",\n",
    "            mode='RGB', imagesorder='RGB', imagesRGB=imagesRGB, noiselums=noiselums, \n",
    "            images=None, outname='color_image_temp', satpercent=0.000, noiselum=0.5, \n",
    "            noisesig0=10, correctbias=0, colorsatfac=1, combine='sum', show=False\n",
    "            ).run()\n",
    "\n",
    "    # read in RGB image and delete png file\n",
    "    im = Image.open('color_image_temp.png')\n",
    "    im = im.transpose(method=Image.FLIP_TOP_BOTTOM)\n",
    "    MIRI_image = np.asarray(im)\n",
    "    os.remove('color_image_temp.png')\n",
    "    \n",
    "    # read WCS from a single FITS file\n",
    "    single_filename = imagesRGB['R'][0].split('[')[0] # remove '[1]' if present\n",
    "    with fits.open(single_filename) as hdul:\n",
    "        im_header = hdul[0].header\n",
    "        im_wcs = WCS(im_header)\n",
    "        \n",
    "        # Extract pixel scale from header\n",
    "        arcsec_per_pixel = np.abs(im_wcs.pixel_scale_matrix[0,0]) * 3600 # degrees to arcsec\n",
    "        print(arcsec_per_pixel)\n",
    "        # Extract rotation angle from PC matrix\n",
    "        if 'PC1_1' in im_header and 'PC2_2' in im_header:\n",
    "            pc11 = im_header['PC1_1'] \n",
    "            angle = 180 - np.arccos(pc11) * 180 / np.pi\n",
    "    \n",
    "    print(f\"Applying a rotation by {angle} degrees to the images.\")\n",
    "    \n",
    "    # Apply rotation and crop to 3 arcsec\n",
    "    MIRI_image = rotate_and_crop_image(im, angle_deg=-angle, crop_arcsec=crop_arcsec)\n",
    "    \n",
    "    if show_channels == True:\n",
    "        # Plot red channel\n",
    "        filename, ext = imagesRGB['R'][0].strip(']').split('[')\n",
    "        with fits.open(filename) as hdul:\n",
    "            hdul.info()\n",
    "            r_data = hdul[0].data\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(r_data, cmap='Reds', origin='lower')\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot green channel\n",
    "        filename, ext = imagesRGB['G'][0].strip(']').split('[')\n",
    "        with fits.open(filename) as hdul:\n",
    "            g_data = hdul[0].data\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(g_data, cmap='Reds', origin='lower')\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot blue channel\n",
    "        filename, ext = imagesRGB['B'][0].strip(']').split('[')\n",
    "        with fits.open(filename) as hdul:\n",
    "            b_data = hdul[0].data\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(b_data, cmap='Blues', origin='lower')\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.imshow(MIRI_image, origin='lower')\n",
    "    ax.axis('off')  # hide the axis coordinates, ticks, and labels\n",
    "    fig.savefig(outfile, bbox_inches='tight', pad_inches=0.0)\n",
    "\n",
    "    \n",
    "def rotate_and_crop_image(image, angle_deg, crop_arcsec, arcsec_per_pixel=0.108):\n",
    "    \"\"\"\n",
    "    Rotate a PIL image and crop to desired sky size (in arcsec).\n",
    "    \"\"\"\n",
    "    rotated = image.rotate(angle_deg, resample=Image.BICUBIC, expand=True)\n",
    "\n",
    "    crop_size_pix = int(crop_arcsec / arcsec_per_pixel)\n",
    "\n",
    "    # Crop centred on the middle\n",
    "    width, height = rotated.size\n",
    "    cx, cy = width // 2, height // 2\n",
    "    half = crop_size_pix // 2\n",
    "\n",
    "    cropped = rotated.crop((cx - half, cy - half, cx + half, cy + half))\n",
    "    \n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Now let's try and call the function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Produce the stamps for PRIMER 003 F1800W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gal_id in f1800w_primer003_ids:\n",
    "    try:\n",
    "        #print(f\"Creating image for {gal_id}...\")\n",
    "        \n",
    "        # Construct base filenames (without [0])\n",
    "        r_file_base = os.path.join(input_dir, f\"{gal_id}_F1800W_cutout_primer003_shifted.fits\")\n",
    "        b_file_primer_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_primer003_shifted.fits\")\n",
    "        b_file_cweb_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_cweb*_shifted.fits\")\n",
    "\n",
    "        # Decide which F770W file to use\n",
    "        if gal_id in f770w_primer003_ids and os.path.exists(b_file_primer_base):\n",
    "            b_file = b_file_primer_base + \"[1]\"\n",
    "        elif gal_id in f770w_cweb1_ids and os.path.exists(b_file_cweb_base):\n",
    "            print(\"Found a galaxy that is mapped by PRIMER 003 and COSMOS-Web 1: \", gal_id)\n",
    "        elif gal_id in f770w_cweb2_ids and os.path.exists(b_file_cweb_base):\n",
    "            b_file = b_file_cweb_base + \"[1]\"\n",
    "        else:\n",
    "            print(f\"[Skipping {gal_id}] No valid F770W file found.\")\n",
    "            continue\n",
    "        \n",
    "        # Now safely add [0] to files\n",
    "        r_file = r_file_base + \"[1]\"\n",
    "        with fits.open(r_file_base) as hdul:\n",
    "            r_data = hdul[1].data\n",
    "            g_data = np.zeros_like(r_data)  # fake green\n",
    "        fits.writeto('fake_green.fits', g_data, overwrite=True)\n",
    "        \n",
    "        # Stack into imagesRGB dictionary\n",
    "        imagesRGB = {'R': [r_file], \n",
    "                    'G': ['fake_green.fits[0]'], \n",
    "                    'B': [b_file]}\n",
    "        \n",
    "        # Call your image function (without shutters)\n",
    "        # Q determines the strength of the non-linearity for the stretch.\n",
    "        # alpha adjusts the sensitivity of the stretch for the given filter.\n",
    "\n",
    "        outfile = os.path.join(output_dir, f\"{gal_id}_primer003.pdf\")\n",
    "        make_stamp(imagesRGB, outfile=outfile)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gal_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Produce the stamps for PRIMER 004 F1800W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gal_id in f1800w_primer004_ids:\n",
    "    try:\n",
    "        #print(f\"Creating image for {gal_id}...\")\n",
    "        \n",
    "        # Construct base filenames (without [0])\n",
    "        r_file_base = os.path.join(input_dir, f\"{gal_id}_F1800W_cutout_primer004_shifted.fits\")\n",
    "        b_file_primer_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_primer004_shifted.fits\")\n",
    "        b_file_cweb_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_cweb*_shifted.fits\")\n",
    "\n",
    "        # Decide which F770W file to use\n",
    "        if gal_id in f1800w_primer004_ids and os.path.exists(b_file_primer_base):\n",
    "            b_file = b_file_primer_base + \"[1]\"\n",
    "        elif gal_id in f770w_cweb1_ids and os.path.exists(b_file_cweb_base):\n",
    "            b_file = b_file_cweb_base + \"[1]\"\n",
    "            print(\"Found a galaxy that is mapped by PRIMER 004 and COSMOS-Web 1: \", gal_id)\n",
    "        elif gal_id in f770w_cweb1_ids and os.path.exists(b_file_cweb_base):\n",
    "            print(\"Found a galaxy that is mapped by PRIMER 004 and COSMOS-Web 2: \", gal_id)\n",
    "        else:\n",
    "            print(f\"[Skipping {gal_id}] No valid F770W file found.\")\n",
    "            continue\n",
    "        \n",
    "        # Now safely add [0] to files\n",
    "        r_file = r_file_base + \"[1]\"\n",
    "        with fits.open(r_file_base) as hdul:\n",
    "            r_data = hdul[1].data\n",
    "            g_data = np.zeros_like(r_data)  # fake green\n",
    "        fits.writeto('fake_green.fits', g_data, overwrite=True)\n",
    "        \n",
    "        # Stack into imagesRGB dictionary\n",
    "        imagesRGB = {'R': [r_file], \n",
    "                    'G': ['fake_green.fits[0]'], \n",
    "                    'B': [b_file]}\n",
    "        \n",
    "        # Call your image function (without shutters)\n",
    "        # Q determines the strength of the non-linearity for the stretch.\n",
    "        # alpha adjusts the sensitivity of the stretch for the given filter.\n",
    "\n",
    "        outfile = os.path.join(output_dir, f\"{gal_id}_primer004.pdf\")\n",
    "        make_stamp(imagesRGB, outfile=outfile)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gal_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Produce stamps for COSMOS-Web in the F770W filters (Obs 1&2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gal_id in f770w_cweb1_ids:\n",
    "    try:\n",
    "        print(f\"Creating image for {gal_id}...\")#\n",
    "        \n",
    "        # Construct base filenames (without [0])\n",
    "        r_file_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_cweb1_shifted.fits\")\n",
    "        \n",
    "        # Now safely add [1] to files\n",
    "        r_file = r_file_base + \"[1]\"\n",
    "        with fits.open(r_file_base) as hdul:\n",
    "            r_data = hdul[1].data\n",
    "            g_data = np.zeros_like(r_data)  # fake green\n",
    "            b_data = np.zeros_like(r_data)  # fake blue\n",
    "        fits.writeto('fake_green.fits', g_data, overwrite=True)\n",
    "        fits.writeto('fake_blue.fits', b_data, overwrite=True)\n",
    "        \n",
    "        # Stack into imagesRGB dictionary\n",
    "        imagesRGB = {'R': [r_file], \n",
    "                    'G': ['fake_green.fits[0]'], \n",
    "                    'B': ['fake_blue.fits[0]']}\n",
    "        \n",
    "        # Call your image function (without shutters)\n",
    "        # Q determines the strength of the non-linearity for the stretch.\n",
    "        # alpha adjusts the sensitivity of the stretch for the given filter.\n",
    "\n",
    "        outfile = os.path.join(output_dir, f\"{gal_id}_cweb1.pdf\")\n",
    "        make_stamp(imagesRGB, outfile=outfile)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gal_id}: {e}\")\n",
    "\n",
    "for gal_id in f770w_cweb2_ids:\n",
    "    try:\n",
    "        print(f\"Creating image for {gal_id}...\")\n",
    "        \n",
    "        # Construct base filenames (without [0])\n",
    "        r_file_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_cweb2_shifted.fits\")\n",
    "        \n",
    "        # Now safely add [1] to files\n",
    "        r_file = r_file_base + \"[1]\"\n",
    "        with fits.open(r_file_base) as hdul:\n",
    "            r_data = hdul[1].data\n",
    "            g_data = np.zeros_like(r_data)  # fake green\n",
    "            b_data = np.zeros_like(r_data)  # fake blue\n",
    "        fits.writeto('fake_green.fits', g_data, overwrite=True)\n",
    "        fits.writeto('fake_blue.fits', b_data, overwrite=True)\n",
    "        \n",
    "        # Stack into imagesRGB dictionary\n",
    "        imagesRGB = {'R': [r_file], \n",
    "                    'G': ['fake_green.fits[0]'], \n",
    "                    'B': ['fake_blue.fits[0]']}\n",
    "        \n",
    "        # Call your image function (without shutters)\n",
    "        # Q determines the strength of the non-linearity for the stretch.\n",
    "        # alpha adjusts the sensitivity of the stretch for the given filter.\n",
    "\n",
    "        outfile = os.path.join(output_dir, f\"{gal_id}_cweb2.pdf\")\n",
    "        make_stamp(imagesRGB, outfile=outfile)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gal_id}: {e}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
