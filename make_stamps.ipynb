{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Make RGB NIRCam stamps showing slit footprint\n",
    "We will be using the `trilogy` package to make RGB stamps and the shutter footprint regions.\n",
    "The information we need to show the footprint and the source (optional) is all in the \"shutters\" files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trilogy import trilogy\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import astropy.units as u\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL  # Python Image Library\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "import contextlib\n",
    "\n",
    "from astropy.nddata import Cutout2D\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Alternative function that produces RGB images from the 5x5\" images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SETUP ----------\n",
    "input_dir = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/\"\n",
    "output_dir = \"/home/bpc/University/master/Red_Cardinal/stamps_miri/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ---------- STEP 1: Get IDs from NIRCam, PRIMER and COSMOS-Web ----------\n",
    "f444w_files = glob.glob(os.path.join(input_dir, \"*F444W*.fits\"))\n",
    "f444w_ids = [os.path.basename(f).split(\"_F444W\")[0] for f in f444w_files]\n",
    "\n",
    "f770w_primer_files = glob.glob(os.path.join(input_dir, \"*F770W_cutout_primer.fits\"))\n",
    "f770w_primer_ids = [os.path.basename(f).split(\"_F770W\")[0] for f in f770w_primer_files]\n",
    "\n",
    "f770w_cweb_files = glob.glob(os.path.join(input_dir, \"*F770W_cutout_cweb.fits\"))\n",
    "f770w_cweb_ids = [os.path.basename(f).split(\"_F770W\")[0] for f in f770w_cweb_files]\n",
    "\n",
    "f1800w_files = glob.glob(os.path.join(input_dir, \"*F1800W*.fits\"))\n",
    "f1800w_ids = [os.path.basename(f).split(\"_F1800W\")[0] for f in f1800w_files]\n",
    "\n",
    "# Remove all galaxies in f770w_primer_ids from f770w_cweb_ids\n",
    "f770w_cweb_ids = np.setdiff1d(f770w_cweb_ids, f770w_primer_ids)\n",
    "f770w_leftover_primer = np.setdiff1d(f770w_primer_ids, f1800w_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Define the new make_stamp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_image(img, stretch='asinh', Q=10, alpha=1, weight=1.0):\n",
    "    \"\"\"\n",
    "    Normalises the input image with optional stretching and channel weighting.\n",
    "\n",
    "    Parameters:\n",
    "    - img: 2D numpy array\n",
    "    - stretch: Type of stretch ('asinh' or 'linear')\n",
    "    - Q: Controls asinh stretch strength\n",
    "    - alpha: Controls non-linearity for asinh\n",
    "    - weight: Multiplier to boost/dampen this channelâ€™s contribution\n",
    "\n",
    "    Returns:\n",
    "    - Normalised image scaled between 0 and 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace nans, positive and negative infinities with 0.0\n",
    "    img = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Clip possible negative fluxes\n",
    "    img = np.clip(img, 0, None)\n",
    "    \n",
    "    #print(f\"Before any scaling: min={np.min(img)}, max={np.max(img)}\")\n",
    "    \n",
    "    # Subtract the minimum value to shift the range to start from 0\n",
    "    img_min = np.min(img)\n",
    "    img -= img_min\n",
    "    #print(f\"After minimum subtraction: min={np.min(img)}, max={np.max(img)}\")\n",
    "\n",
    "    \n",
    "    #print(f\"After weight scaling: min={np.min(img)}, max={np.max(img)}\")\n",
    "    \n",
    "    # Determine which scaling to us\n",
    "    if stretch == 'asinh':  # Lupton scaling\n",
    "        img_scaled = np.arcsinh(alpha * Q * img) / Q\n",
    "    elif stretch == 'log':\n",
    "        img_scaled = np.log10(1 + alpha * img)\n",
    "    elif stretch == 'linear':\n",
    "        img_scaled = img\n",
    "    else:\n",
    "        raise ValueError(\"Unknown stretch\")\n",
    "        \n",
    "    #print(f\"After stretch: min={np.min(img_scaled)}, max={np.max(img_scaled)}\")\n",
    "    \n",
    "    # After stretching the image is normalised to 1\n",
    "    img_scaled = img_scaled / np.nanmax(img_scaled) if np.nanmax(img_scaled) != 0 else img_scaled\n",
    "    \n",
    "    #print(f\"After normalisation: min={np.min(img_scaled)}, max={np.max(img_scaled)}\")\n",
    "    \n",
    "    return img_scaled\n",
    "\n",
    "\n",
    "def preprocess_fits_image(filename, ext=0, stretch='asinh', Q=10, alpha=1, weight=1, normalise=False):\n",
    "    \"\"\"\n",
    "    Load, optionally resample, and normalise a FITS image.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: FITS filename with optional extension (e.g., 'file.fits[1]')\n",
    "    - stretch, Q, alpha, weight: Passed to normalize_image\n",
    "    - upscale_size: Tuple of (new_y, new_x) size to resize image to\n",
    "\n",
    "    Returns:\n",
    "    - Processed 2D numpy image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fits.open(filename) as hdul:\n",
    "            img = hdul[ext].data.astype(float)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Could not open {filename}[{ext}]: {e}\")\n",
    "\n",
    "    # Remove NaNs and negative values for display purposes\n",
    "    flat = img.flatten()\n",
    "    flat = flat[np.isfinite(flat)]  # remove NaNs/Infs\n",
    "\n",
    "    # Sort pixel values\n",
    "    sorted_pixels = np.sort(flat)\n",
    "\n",
    "    # Get the lower X% of the pixels\n",
    "    cutoff_index = int(len(sorted_pixels) * 0.8)\n",
    "    faint_pixels = sorted_pixels[:cutoff_index]\n",
    "\n",
    "    # Compute robust mean\n",
    "    background_mean = np.mean(faint_pixels)\n",
    "    background_mean = np.nanmean(img)\n",
    "    img = np.where(np.isnan(img), background_mean, img)\n",
    "    img -= np.min(img)\n",
    "    \n",
    "    # Ensure all values are non-negative before stretch\n",
    "    img[img < 0] = 0.0\n",
    "\n",
    "    # Apply stretch\n",
    "    if stretch == 'asinh':\n",
    "        img = np.arcsinh(Q * alpha * img) / Q\n",
    "    elif stretch == 'linear':\n",
    "        pass  # No stretch applied\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported stretch: {stretch}\")\n",
    "\n",
    "    # Normalise if requested\n",
    "    if normalise:\n",
    "        max_val = np.nanmax(img)\n",
    "        if max_val > 0:\n",
    "            img /= max_val\n",
    "\n",
    "    # Clip just in case\n",
    "    #img = np.clip(img, 0, 1)\n",
    "\n",
    "    return img\n",
    "    \n",
    "\n",
    "def make_stamp(imagesRGB, Q_r, alpha_r, weight_r, Q_g, alpha_g, weight_g, Q_b, alpha_b, weight_b=1.0, stretch='asinh', outfile='stamp.pdf'):\n",
    "    \"\"\"\n",
    "    Make RGB stamp using trilogy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameter dictionary to handle values per channel\n",
    "    params = {\n",
    "        'R': {'Q': Q_r, 'alpha': alpha_r, 'weight': weight_r},\n",
    "        'G': {'Q': Q_g, 'alpha': alpha_g, 'weight': weight_g},\n",
    "        'B': {'Q': Q_b, 'alpha': alpha_b, 'weight': weight_b}\n",
    "    }\n",
    "    \n",
    "    stretched_images = {}\n",
    "    global_max = 0.0\n",
    "\n",
    "    # --- Section for normalising the images ---\n",
    "\n",
    "    # Stretch each image and find the global max\n",
    "    for colour in ['R', 'G', 'B']:\n",
    "        image_str = imagesRGB[colour][0]\n",
    "        fname, ext = image_str.split('[')\n",
    "        ext = int(ext.replace(']', ''))\n",
    "        colour_params = params[colour]\n",
    "\n",
    "        # Stretch but don't normalise yet\n",
    "        stretched = preprocess_fits_image(\n",
    "            fname,\n",
    "            ext,\n",
    "            stretch=stretch,\n",
    "            Q=colour_params['Q'],\n",
    "            alpha=colour_params['alpha'],\n",
    "            normalise=False  # You'll need to add this option in your function\n",
    "        )\n",
    "\n",
    "        stretched_images[colour] = stretched\n",
    "        max_val = np.nanmax(stretched)\n",
    "        if max_val > global_max:\n",
    "            global_max = max_val\n",
    "    \n",
    "    # Now normalise to global max\n",
    "    norm_images = {}\n",
    "    for colour in ['B', 'G', 'R']:\n",
    "        norm_images[colour] = stretched_images[colour] / global_max\n",
    "        norm_images[colour] = np.clip(norm_images[colour], 0, 1)\n",
    "\n",
    "    # Add scaled images to the dictionary\n",
    "    temp_files = {}\n",
    "    for colour in norm_images:\n",
    "        fname = f'temp_{colour}.fits'\n",
    "        fits.writeto(fname, norm_images[colour], overwrite=True)\n",
    "        temp_files[colour] = fname   \n",
    "    \n",
    "    for colour in norm_images:\n",
    "        # Replace remaining NaNs just before stacking\n",
    "        norm_images[colour] = np.nan_to_num(norm_images[colour], nan=0.0)\n",
    "        norm_images[colour] = np.clip(norm_images[colour], 0, 1)\n",
    "\n",
    "    # --- Section to check whether all filters are available for RGB mapping ---    \n",
    "    \n",
    "    if 'fake' in imagesRGB['G'][0]:     # only R and B available\n",
    "        # Create a horizontal panel with 2 grayscale subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # 2 images side-by-side\n",
    "\n",
    "        # Map colour channels to filters for labelling\n",
    "        channel_labels = {'R': 'F770W', 'B': 'F444W'}\n",
    "        \n",
    "        # Legend info\n",
    "        legend_text = f\"R: {os.path.basename(imagesRGB['R'][0]).split('_')[1]}\\n\" \\\n",
    "                      f\"B: {os.path.basename(imagesRGB['B'][0]).split('_')[1]}\"\n",
    "\n",
    "        for ax, colour in zip(axes, ['B', 'R']):\n",
    "            ax.imshow(norm_images[colour], cmap='gray', origin='lower')\n",
    "            ax.set_title(channel_labels[colour], fontsize=12)\n",
    "            ax.axis('off')       \n",
    "        \n",
    "    else:   # R, G and B available\n",
    "        # Create a horizontal panel with 3 grayscale subplots\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # 3 images side-by-side\n",
    "\n",
    "        # Map colour channels to filters for labelling\n",
    "        channel_labels = {'R': 'F1800W', 'G': 'F770W', 'B': 'F444W'}\n",
    "\n",
    "        # Legend info\n",
    "        legend_text = f\"R: {os.path.basename(imagesRGB['R'][0]).split('_')[1]}\\n\" \\\n",
    "                      f\"G: {os.path.basename(imagesRGB['G'][0]).split('_')[1]}\\n\" \\\n",
    "                      f\"B: {os.path.basename(imagesRGB['B'][0]).split('_')[1]}\"\n",
    "\n",
    "        for ax, colour in zip(axes, ['B', 'G', 'R']):\n",
    "            ax.imshow(norm_images[colour], cmap='gray', origin='lower')\n",
    "            ax.set_title(channel_labels[colour], fontsize=12)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    greyscale = outfile.replace('.pdf', '_filters.pdf')\n",
    "    plt.savefig(greyscale, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "    # --- Plot using matplotlib and save ---\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    rgb_image = np.stack([norm_images['R'], norm_images['G'], norm_images['B']], axis=-1)\n",
    "    ax.imshow(rgb_image, origin='lower')\n",
    "    #ax.imshow(NIRCam_image, origin='lower')\n",
    "    ax.text(0.02, 0.98, legend_text, transform=ax.transAxes,\n",
    "            fontsize=10, va='top', ha='left',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "    ax.axis('off')  # Hide axes\n",
    "    \n",
    "    # Save the RGB image as PDF\n",
    "    fig.savefig(outfile, bbox_inches='tight', pad_inches=0.0)\n",
    "    plt.close(fig)  # Clean up the figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Now let's try and call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gal_id in f1800w_ids:\n",
    "    try:\n",
    "        print(f\"Creating image for {gal_id}...\")\n",
    "        \n",
    "        # Construct base filenames (without [0])\n",
    "        r_file_base = os.path.join(input_dir, f\"{gal_id}_F1800W_cutout_primer.fits\")\n",
    "        g_file_primer_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_primer.fits\")\n",
    "        g_file_cweb_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_cweb.fits\")\n",
    "        b_file_base = os.path.join(input_dir, f\"{gal_id}_F444W_cutout_nircam.fits\")\n",
    "\n",
    "        # Decide which F770W file to use\n",
    "        if gal_id in f770w_primer_ids and os.path.exists(g_file_primer_base):\n",
    "            g_file = g_file_primer_base + \"[1]\"\n",
    "        elif gal_id in f770w_cweb_ids and os.path.exists(g_file_cweb_base):\n",
    "            g_file = g_file_cweb_base + \"[1]\"\n",
    "        else:\n",
    "            print(f\"[Skipping {gal_id}] No valid F770W file found.\")\n",
    "            continue\n",
    "\n",
    "        # Now safely add [0] to files\n",
    "        r_file = r_file_base + \"[1]\"\n",
    "        b_file = b_file_base + \"[0]\"\n",
    "        \n",
    "        # Stack into imagesRGB dictionary\n",
    "        imagesRGB = {'R': [r_file], \n",
    "                    'G': [g_file], \n",
    "                    'B': [b_file]}\n",
    "        \n",
    "        # Call your image function (without shutters)\n",
    "        outfile = os.path.join(output_dir, f\"{gal_id}_primer.pdf\")\n",
    "        if gal_id == '19098':\n",
    "            make_stamp(imagesRGB, outfile=outfile, \n",
    "                    Q_r=10, alpha_r=1, weight_r=1,\n",
    "                    Q_g=10, alpha_g=1, weight_g=1,\n",
    "                    Q_b=10, alpha_b=5, weight_b=1,\n",
    "                    stretch='asinh')\n",
    "            \n",
    "        else:\n",
    "            make_stamp(imagesRGB, outfile=outfile, \n",
    "                    Q_r=5, alpha_r=0.7, weight_r=1,\n",
    "                    Q_g=5, alpha_g=0.85, weight_g=1,\n",
    "                    Q_b=10, alpha_b=1.0, weight_b=1,\n",
    "                    stretch='asinh')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gal_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Produce the stamps for all PRIMER galaxies that were only mapped in F770W AND for COSMOS-Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gal_id in f770w_leftover_primer:\n",
    "    try:\n",
    "        print(f\"Creating image for {gal_id}...\")\n",
    "        \n",
    "        # Construct base filenames (without [0])\n",
    "        r_file_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_primer.fits\")\n",
    "        b_file_base = os.path.join(input_dir, f\"{gal_id}_F444W_cutout_nircam.fits\")\n",
    "\n",
    "        # Now safely add [1] to files\n",
    "        r_file = r_file_base + \"[1]\"\n",
    "        b_file = b_file_base + \"[0]\"\n",
    "        \n",
    "        with fits.open(r_file_base) as hdul:\n",
    "            r_data = hdul[1].data\n",
    "            g_data = np.zeros_like(r_data)  # fake green\n",
    "        fits.writeto('fake_green.fits', g_data, overwrite=True)\n",
    "        \n",
    "        # Stack into imagesRGB dictionary\n",
    "        imagesRGB = {'R': [r_file], \n",
    "                    'G': ['fake_green.fits[0]'], \n",
    "                    'B': [b_file]}\n",
    "        \n",
    "        # Call your image function (without shutters)\n",
    "        # Q determines the strength of the non-linearity for the stretch.\n",
    "        # alpha adjusts the sensitivity of the stretch for the given filter.\n",
    "\n",
    "        outfile = os.path.join(output_dir, f\"{gal_id}_primer.pdf\")\n",
    "        make_stamp(imagesRGB, outfile=outfile, \n",
    "                Q_r=5, alpha_r=0.7, weight_r=1,\n",
    "                Q_g=5, alpha_g=0.85, weight_g=1,\n",
    "                Q_b=10, alpha_b=1.0, weight_b=1,\n",
    "                stretch='asinh')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gal_id}: {e}\")\n",
    "\n",
    "for gal_id in f770w_cweb_ids:\n",
    "    try:\n",
    "        print(f\"Creating image for {gal_id}...\")\n",
    "        \n",
    "        # Construct base filenames (without [0])\n",
    "        r_file_base = os.path.join(input_dir, f\"{gal_id}_F770W_cutout_cweb.fits\")\n",
    "        b_file_base = os.path.join(input_dir, f\"{gal_id}_F444W_cutout_nircam.fits\")\n",
    "\n",
    "        # Now safely add [1] to files\n",
    "        r_file = r_file_base + \"[1]\"\n",
    "        b_file = b_file_base + \"[0]\"\n",
    "        \n",
    "        with fits.open(r_file_base) as hdul:\n",
    "            r_data = hdul[1].data\n",
    "            g_data = np.zeros_like(r_data)  # fake green\n",
    "        fits.writeto('fake_green.fits', g_data, overwrite=True)\n",
    "        \n",
    "        # Stack into imagesRGB dictionary\n",
    "        imagesRGB = {'R': [r_file], \n",
    "                    'G': ['fake_green.fits[0]'], \n",
    "                    'B': [b_file]}\n",
    "        \n",
    "        # Call your image function (without shutters)\n",
    "        # Q determines the strength of the non-linearity for the stretch.\n",
    "        # alpha adjusts the sensitivity of the stretch for the given filter.\n",
    "\n",
    "        outfile = os.path.join(output_dir, f\"{gal_id}_cweb.pdf\")\n",
    "        make_stamp(imagesRGB, outfile=outfile, \n",
    "                Q_r=5, alpha_r=0.7, weight_r=1,\n",
    "                Q_g=5, alpha_g=0.85, weight_g=1,\n",
    "                Q_b=10, alpha_b=1.0, weight_b=1,\n",
    "                stretch='asinh')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gal_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Check extensions in cutout FITS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_file = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/18769_F1800W_cutout_primer.fits\"\n",
    "with fits.open(r_file) as hdul:\n",
    "    for i, hdu in enumerate(hdul):\n",
    "        print(f\"HDU {i}: {hdu.name}, shape = {getattr(hdu.data, 'shape', None)}\")\n",
    "\n",
    "g_file = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/18769_F770W_cutout_primer.fits\"\n",
    "with fits.open(g_file) as hdul:\n",
    "    for i, hdu in enumerate(hdul):\n",
    "        print(f\"HDU {i}: {hdu.name}, shape = {getattr(hdu.data, 'shape', None)}\")\n",
    "        \n",
    "b_file = \"/home/bpc/University/master/Red_Cardinal/cutouts_3x3/18769_F444W_cutout_nircam.fits\"\n",
    "with fits.open(b_file) as hdul:\n",
    "    for i, hdu in enumerate(hdul):\n",
    "        print(f\"HDU {i}: {hdu.name}, shape = {getattr(hdu.data, 'shape', None)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Now let's check if we got all the galaxies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(input_dir, \"*.fits\"))\n",
    "unique_ids = []\n",
    "\n",
    "for file in files:\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    if \"nircam\" in filename:     \n",
    "        continue\n",
    "    \n",
    "    id = filename.split(\"_\")[0]\n",
    "    if id not in unique_ids:\n",
    "        unique_ids.append(id)\n",
    "print(unique_ids)\n",
    "print(len(unique_ids))\n",
    "\n",
    "stamps = glob.glob(os.path.join(output_dir, \"*.pdf\"))\n",
    "unique_stamps = []\n",
    "\n",
    "for stamp in stamps:\n",
    "    stampname = os.path.basename(stamp)\n",
    "    id = stampname.split(\"_\")[0]\n",
    "    if id not in unique_stamps:\n",
    "        unique_stamps.append(id)\n",
    "\n",
    "print(unique_stamps)\n",
    "print(len(unique_stamps))\n",
    "\n",
    "for gal in f770w_leftover_primer:\n",
    "    if gal in f770w_cweb_ids:\n",
    "        print(gal)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
